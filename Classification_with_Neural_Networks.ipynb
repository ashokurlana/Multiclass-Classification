{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "strategic-leone"
      },
      "source": [
        "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn import decomposition, ensemble\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import pandas, numpy, string\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras import layers, models, optimizers\n",
        "from sklearn import model_selection, svm\n",
        "import pandas as pd\n",
        "from keras.utils import np_utils\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import numpy as np\n",
        "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n"
      ],
      "id": "strategic-leone",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biological-punishment"
      },
      "source": [
        "trainDF = pd.read_csv('data.csv', delimiter=',')\n",
        "trainDF.head()"
      ],
      "id": "biological-punishment",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "global-badge"
      },
      "source": [
        "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['text'], trainDF['category'],test_size=0.15,random_state=42)"
      ],
      "id": "global-badge",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "occasional-michael",
        "outputId": "728c7463-7d7a-4062-8ebc-3f5a06538008"
      },
      "source": [
        "trainDF['category'].value_counts()"
      ],
      "id": "occasional-michael",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Employement Agreement     57\n",
              "Restricted Stock Units    40\n",
              "Incentive Plan            40\n",
              "Compensation Agreement    20\n",
              "Release and separation    18\n",
              "Promissory Note           12\n",
              "Service Agreement         12\n",
              "Lease Agreement           11\n",
              "Purchase Agreement        10\n",
              "Loan Agreement             9\n",
              "Credit Agreement           9\n",
              "change of control          7\n",
              "License Agreement          7\n",
              "consulting agreement       7\n",
              "Subscription Agreement     6\n",
              "Name: category, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "varied-psychology",
        "outputId": "b1f1bdba-dd2c-4310-b100-8508260cb207"
      },
      "source": [
        "# labelEncoder = preprocessing.LabelEncoder()\n",
        "# labelEncoder.fit(['Employement Agreement','Restricted Stock Units','Incentive Plan','Compensation Agreement','Release and separation','Promissory Note','Service Agreement','Lease Agreement', 'Purchase Agreement', 'Loan Agreement','Credit Agreement', 'change of control','License Agreement', 'consulting agreement','Subscription Agreement'   ])\n",
        "# #numericalLabels_train = labelEncoder.transform(train_y)\n",
        "# #numericalLabels_valid = labelEncoder.transform(valid_y)\n",
        "\n",
        "# encoder = preprocessing.LabelEncoder()\n",
        "# trainLabels = encoder.fit_transform(train_y)\n",
        "# trainLabels = [np_utils.to_categorical(i, num_classes=9) for i in trainLabels]\n",
        "# trainLabels = np.asarray(trainLabels)\n",
        "# print(trainLabels.shape)\n",
        "# validLabels = encoder.fit_transform(valid_y)\n",
        "# validLabels = [np_utils.to_categorical(i, num_classes=9) for i in validLabels]\n",
        "# validLabels = np.asarray(validLabels)\n",
        "# print(validLabels.shape)\n",
        "\n",
        "\n",
        "labelEncoder = preprocessing.LabelEncoder()\n",
        "#labelEncoder.fit(['PER','LOC','DAT','CAR','ORG','QUA','CUR','PRC', 'TIM'])\n",
        "labelEncoder.fit(['Employement Agreement','Restricted Stock Units','Incentive Plan','Compensation Agreement','Release and separation','Promissory Note','Service Agreement','Lease Agreement', 'Purchase Agreement', 'Loan Agreement','Credit Agreement', 'change of control','License Agreement', 'consulting agreement','Subscription Agreement'])\n",
        "numericalLabels_train = labelEncoder.transform(train_y)\n",
        "numericalLabels_valid = labelEncoder.transform(valid_y)\n",
        "\n",
        "encoder = preprocessing.LabelEncoder()\n",
        "trainLabels = encoder.fit_transform(train_y)\n",
        "trainLabels = [np_utils.to_categorical(i, num_classes=15) for i in trainLabels]\n",
        "trainLabels = np.asarray(trainLabels)\n",
        "print(trainLabels.shape)\n",
        "validLabels = encoder.fit_transform(valid_y)\n",
        "validLabels = [np_utils.to_categorical(i, num_classes=15) for i in validLabels]\n",
        "validLabels = np.asarray(validLabels)\n",
        "print(validLabels.shape)"
      ],
      "id": "varied-psychology",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(225, 15)\n",
            "(40, 15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27IoFx7RVtKT"
      },
      "source": [
        "# Get the FastText Telugu word embeddigns\n",
        "#!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.te.300.vec.gz\n",
        "#!gunzip /content/cc.te.300.vec.gz\n",
        "#!mv /content/cc.te.300.vec '/content/drive/MyDrive/Question Classifier Telugu/'"
      ],
      "id": "27IoFx7RVtKT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "interesting-poetry"
      },
      "source": [
        "# Using FastText pre trained telugu embeddings\n",
        "embeddings_index = {}\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from keras.preprocessing import text, sequence\n",
        "# Provide the embeddings for the particluar language\n",
        "for i, line in enumerate(open('wiki-news-300d-1M.vec',encoding=\"utf-8\")):\n",
        "    values = line.split()\n",
        "    embeddings_index[values[0]] = np.asarray(values[1:], dtype='float32')\n",
        "    "
      ],
      "id": "interesting-poetry",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "strong-organizer"
      },
      "source": [
        "token = text.Tokenizer()\n",
        "token.fit_on_texts(trainDF['text'])\n",
        "word_index = token.word_index\n",
        "#print(token.word_index)"
      ],
      "id": "strong-organizer",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "norwegian-prime",
        "outputId": "3c137906-4c99-4402-a3f4-05c5890bb43b"
      },
      "source": [
        "#print(word_index)\n",
        "input_size = 150\n",
        "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x),maxlen=input_size)\n",
        "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x),maxlen=input_size)\n",
        "print(train_seq_x)\n",
        "print(train_seq_x.shape)\n",
        "print(valid_seq_x.shape)"
      ],
      "id": "norwegian-prime",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[   44   463   489 ...   344   141     5]\n",
            " [    1   535    17 ...    14    15  5543]\n",
            " [12411     5   753 ...   303   559     5]\n",
            " ...\n",
            " [    2     8     2 ...    14    15  1306]\n",
            " [    3    32  1674 ...  3754   787     5]\n",
            " [   20   152   399 ...     6    86   389]]\n",
            "(225, 150)\n",
            "(40, 150)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "convertible-jerusalem",
        "outputId": "747a586b-870e-4f72-d715-af2bf8028857"
      },
      "source": [
        "embedding_matrix = np.zeros((len(word_index)+1, 300))\n",
        "\n",
        "for word,i in word_index.items():\n",
        "        embedding_vector = embeddings_index.get(word)    # checking that particular indexed word in telugu embedding .vec file\n",
        "        if embedding_vector is not None:                 # if it is found in that .vec file  \n",
        "            embedding_matrix[i] = embedding_vector       # store that vector of that word in .vec file saved to embedding matrix\n",
        "               \n",
        "print(embedding_matrix)  \n",
        "print(embedding_matrix.shape)"
      ],
      "id": "convertible-jerusalem",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.      0.      0.     ...  0.      0.      0.    ]\n",
            " [ 0.0897  0.016  -0.0571 ...  0.1559 -0.0254 -0.0259]\n",
            " [-0.0063 -0.0253 -0.0338 ...  0.1155  0.0073  0.0168]\n",
            " ...\n",
            " [-0.0111 -0.0381 -0.0108 ...  0.2289 -0.0134  0.0808]\n",
            " [ 0.      0.      0.     ...  0.      0.      0.    ]\n",
            " [-0.0612 -0.0729 -0.0567 ... -0.2237 -0.0349  0.1003]]\n",
            "(15064, 300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "german-cathedral"
      },
      "source": [
        "def train_model(classifier, feature_vector_train, category, feature_vector_valid):\n",
        "    classifier.fit(feature_vector_train, category)\n",
        "    predictions = classifier.predict(feature_vector_valid)\n",
        "    acc = metrics.accuracy_score(predictions,valid_y)\n",
        "    #f1Score = metrics.f1_score(predictions, valid_y, average='macro')\n",
        "    #classificationReport = classification_report(predictions, valid_y)\n",
        "    # fit the training dataset on the classifier\n",
        "    return acc    "
      ],
      "id": "german-cathedral",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0806M2gs2GL"
      },
      "source": [
        "import inspect\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "id": "L0806M2gs2GL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN"
      ],
      "metadata": {
        "id": "fZndSJZJSOvZ"
      },
      "id": "fZndSJZJSOvZ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "virgin-brooklyn"
      },
      "source": [
        "def create_cnn():\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((input_size, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.25)(embedding_layer)\n",
        "\n",
        "    #Add the convolutional layer\n",
        "    conv_layer = layers.Convolution1D(256, 3, activation=\"tanh\")(embedding_layer)\n",
        "    \n",
        "    #Add the pooling layer\n",
        "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
        "    \n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(128, activation=\"tanh\")(pooling_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(15, activation=\"softmax\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=Adam(), loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "    #model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    #model.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=['accuracy'])\n",
        "    print(model.summary())\n",
        "    return model"
      ],
      "id": "virgin-brooklyn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chinese-gentleman"
      },
      "source": [
        "classifier = create_cnn()\n",
        "is_neural_net = True\n",
        "classifier.fit(train_seq_x, trainLabels, epochs=2)\n",
        "    \n",
        "    # predict the labels on validation dataset\n",
        "predictions = classifier.predict(valid_seq_x)\n",
        "predictions1 = predictions.argmax(axis=-1)\n",
        "validLabels1 = validLabels.argmax(axis=-1)\n",
        "\n",
        "acc = metrics.accuracy_score(predictions1, validLabels1)\n",
        "#f1Score = metrics.f1_score(predictions1, validLabels,average='macro')\n",
        "#classificationReport = classification_report(predictions1, validLabels)\n",
        "print(acc)\n",
        "#print(f1Score)\n",
        "\n",
        "#print(predictions1)\n",
        "\n",
        "#if is_neural_net:\n",
        "    #predictions = predictions.argmax(axis=-1)\n",
        "    \n",
        "#print(metrics.accuracy_score(predictions, valid_y))\n",
        "#print(metrics.classification_report (valid_y, np.argmax(predictions, axis = 1)))"
      ],
      "id": "chinese-gentleman",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNN LSTM"
      ],
      "metadata": {
        "id": "xmT8RAgLSSzZ"
      },
      "id": "xmT8RAgLSSzZ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "narrative-terror"
      },
      "source": [
        "def create_rnn_lstm():\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((150, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "\n",
        "    # Add the LSTM Layer\n",
        "    lstm_layer = layers.LSTM(150)(embedding_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(150, activation=\"tanh\")(lstm_layer)\n",
        "    #output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(15, activation=\"softmax\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=Adam(), loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "    print(model.summary())\n",
        "    return model"
      ],
      "id": "narrative-terror",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smooth-restaurant",
        "outputId": "3c61c1a7-0b47-4a8c-90db-178489ee4061"
      },
      "source": [
        "classifier = create_rnn_lstm()\n",
        "is_neural_net=True  \n",
        "classifier.fit(train_seq_x, trainLabels, epochs=10)\n",
        "    \n",
        "    # predict the labels on validation dataset\n",
        "predictions = classifier.predict(valid_seq_x)\n",
        "predictions1 = predictions.argmax(axis=-1)\n",
        "validLabels1 = validLabels.argmax(axis=-1)\n",
        "\n",
        "acc = metrics.accuracy_score(predictions1, validLabels1)\n",
        "#f1Score = metrics.f1_score(predictions1, validLabels,average='macro')\n",
        "#classificationReport = classification_report(predictions1, validLabels)\n",
        "\n",
        "print(acc)\n",
        "#print(f1Score)\n"
      ],
      "id": "smooth-restaurant",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_6 (InputLayer)         [(None, 150)]             0         \n",
            "_________________________________________________________________\n",
            "embedding_5 (Embedding)      (None, 150, 300)          4519200   \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d_5 (Spatial (None, 150, 300)          0         \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 150)               270600    \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 150)               22650     \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 15)                2265      \n",
            "=================================================================\n",
            "Total params: 4,814,715\n",
            "Trainable params: 295,515\n",
            "Non-trainable params: 4,519,200\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "8/8 [==============================] - 5s 293ms/step - loss: 2.6119 - accuracy: 0.1422\n",
            "Epoch 2/10\n",
            "8/8 [==============================] - 2s 291ms/step - loss: 2.3921 - accuracy: 0.2400\n",
            "Epoch 3/10\n",
            "8/8 [==============================] - 2s 291ms/step - loss: 2.3474 - accuracy: 0.2311\n",
            "Epoch 4/10\n",
            "8/8 [==============================] - 2s 284ms/step - loss: 2.2826 - accuracy: 0.2000\n",
            "Epoch 5/10\n",
            "8/8 [==============================] - 2s 285ms/step - loss: 2.2726 - accuracy: 0.2044\n",
            "Epoch 6/10\n",
            "8/8 [==============================] - 2s 282ms/step - loss: 2.2781 - accuracy: 0.2267\n",
            "Epoch 7/10\n",
            "8/8 [==============================] - 2s 284ms/step - loss: 2.1850 - accuracy: 0.2978\n",
            "Epoch 8/10\n",
            "8/8 [==============================] - 2s 284ms/step - loss: 2.1555 - accuracy: 0.3022\n",
            "Epoch 9/10\n",
            "8/8 [==============================] - 2s 281ms/step - loss: 2.1261 - accuracy: 0.3111\n",
            "Epoch 10/10\n",
            "8/8 [==============================] - 2s 280ms/step - loss: 2.1180 - accuracy: 0.2844\n",
            "0.125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bidirectional RNN"
      ],
      "metadata": {
        "id": "38Bm4EcuSW3P"
      },
      "id": "38Bm4EcuSW3P"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "concrete-allowance"
      },
      "source": [
        "def create_bidirectional_rnn():\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((150, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "\n",
        "    # Add the LSTM Layer\n",
        "    lstm_layer = layers.Bidirectional(layers.GRU(100))(embedding_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(150, activation=\"tanh\")(lstm_layer)\n",
        "    #output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(15, activation=\"softmax\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=Adam(), loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "    print(model.summary())\n",
        "    return model\n"
      ],
      "id": "concrete-allowance",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "affiliated-borough",
        "outputId": "8a123271-8868-46db-d092-7d2380899a5e"
      },
      "source": [
        "classifier = create_bidirectional_rnn()\n",
        "is_neural_net=True  \n",
        "classifier.fit(train_seq_x, trainLabels, epochs=50)\n",
        "    \n",
        "    # predict the labels on validation dataset\n",
        "predictions = classifier.predict(valid_seq_x)\n",
        "predictions1 = predictions.argmax(axis=-1)\n",
        "validLabels1 = validLabels.argmax(axis=-1)\n",
        "\n",
        "acc = metrics.accuracy_score(predictions1, validLabels1)\n",
        "f1Score = metrics.f1_score(predictions1, validLabels1,average='weighted')\n",
        "classificationReport = classification_report(predictions1, validLabels1)\n",
        "\n",
        "print(acc)\n",
        "print(f1Score)\n",
        "print(classificationReport)\n"
      ],
      "id": "affiliated-borough",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_8 (InputLayer)         [(None, 150)]             0         \n",
            "_________________________________________________________________\n",
            "embedding_7 (Embedding)      (None, 150, 300)          4519200   \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d_7 (Spatial (None, 150, 300)          0         \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 200)               241200    \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 150)               30150     \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 15)                2265      \n",
            "=================================================================\n",
            "Total params: 4,792,815\n",
            "Trainable params: 273,615\n",
            "Non-trainable params: 4,519,200\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/50\n",
            "8/8 [==============================] - 6s 276ms/step - loss: 2.5976 - accuracy: 0.1778\n",
            "Epoch 2/50\n",
            "8/8 [==============================] - 2s 272ms/step - loss: 2.4038 - accuracy: 0.2089\n",
            "Epoch 3/50\n",
            "8/8 [==============================] - 2s 275ms/step - loss: 2.3434 - accuracy: 0.2133\n",
            "Epoch 4/50\n",
            "8/8 [==============================] - 2s 277ms/step - loss: 2.2879 - accuracy: 0.2622\n",
            "Epoch 5/50\n",
            "8/8 [==============================] - 2s 273ms/step - loss: 2.2362 - accuracy: 0.3067\n",
            "Epoch 6/50\n",
            "8/8 [==============================] - 2s 275ms/step - loss: 2.1834 - accuracy: 0.2578\n",
            "Epoch 7/50\n",
            "8/8 [==============================] - 2s 273ms/step - loss: 2.1216 - accuracy: 0.2889\n",
            "Epoch 8/50\n",
            "8/8 [==============================] - 2s 275ms/step - loss: 2.0237 - accuracy: 0.3644\n",
            "Epoch 9/50\n",
            "8/8 [==============================] - 2s 271ms/step - loss: 1.9630 - accuracy: 0.3467\n",
            "Epoch 10/50\n",
            "8/8 [==============================] - 2s 271ms/step - loss: 1.8898 - accuracy: 0.3822\n",
            "Epoch 11/50\n",
            "8/8 [==============================] - 2s 274ms/step - loss: 1.8340 - accuracy: 0.3733\n",
            "Epoch 12/50\n",
            "8/8 [==============================] - 2s 277ms/step - loss: 1.7991 - accuracy: 0.4044\n",
            "Epoch 13/50\n",
            "8/8 [==============================] - 2s 274ms/step - loss: 1.6591 - accuracy: 0.5289\n",
            "Epoch 14/50\n",
            "8/8 [==============================] - 2s 276ms/step - loss: 1.5615 - accuracy: 0.5156\n",
            "Epoch 15/50\n",
            "8/8 [==============================] - 2s 276ms/step - loss: 1.5125 - accuracy: 0.5422\n",
            "Epoch 16/50\n",
            "8/8 [==============================] - 2s 276ms/step - loss: 1.4341 - accuracy: 0.5511\n",
            "Epoch 17/50\n",
            "8/8 [==============================] - 2s 272ms/step - loss: 1.3775 - accuracy: 0.5467\n",
            "Epoch 18/50\n",
            "8/8 [==============================] - 2s 283ms/step - loss: 1.2732 - accuracy: 0.5867\n",
            "Epoch 19/50\n",
            "8/8 [==============================] - 2s 272ms/step - loss: 1.1743 - accuracy: 0.6711\n",
            "Epoch 20/50\n",
            "8/8 [==============================] - 2s 273ms/step - loss: 1.1337 - accuracy: 0.6711\n",
            "Epoch 21/50\n",
            "8/8 [==============================] - 2s 270ms/step - loss: 1.0272 - accuracy: 0.7378\n",
            "Epoch 22/50\n",
            "8/8 [==============================] - 2s 273ms/step - loss: 0.9822 - accuracy: 0.7600\n",
            "Epoch 23/50\n",
            "8/8 [==============================] - 2s 272ms/step - loss: 0.9044 - accuracy: 0.7422\n",
            "Epoch 24/50\n",
            "8/8 [==============================] - 2s 272ms/step - loss: 0.7792 - accuracy: 0.8356\n",
            "Epoch 25/50\n",
            "8/8 [==============================] - 2s 276ms/step - loss: 0.7417 - accuracy: 0.7956\n",
            "Epoch 26/50\n",
            "8/8 [==============================] - 2s 272ms/step - loss: 0.7391 - accuracy: 0.8311\n",
            "Epoch 27/50\n",
            "8/8 [==============================] - 2s 275ms/step - loss: 0.6070 - accuracy: 0.8933\n",
            "Epoch 28/50\n",
            "8/8 [==============================] - 2s 271ms/step - loss: 0.5531 - accuracy: 0.8533\n",
            "Epoch 29/50\n",
            "8/8 [==============================] - 2s 271ms/step - loss: 0.4939 - accuracy: 0.8978\n",
            "Epoch 30/50\n",
            "8/8 [==============================] - 2s 276ms/step - loss: 0.4660 - accuracy: 0.9022\n",
            "Epoch 31/50\n",
            "8/8 [==============================] - 2s 273ms/step - loss: 0.4382 - accuracy: 0.9022\n",
            "Epoch 32/50\n",
            "8/8 [==============================] - 2s 278ms/step - loss: 0.3741 - accuracy: 0.9244\n",
            "Epoch 33/50\n",
            "8/8 [==============================] - 2s 274ms/step - loss: 0.3308 - accuracy: 0.9467\n",
            "Epoch 34/50\n",
            "8/8 [==============================] - 2s 274ms/step - loss: 0.3523 - accuracy: 0.9244\n",
            "Epoch 35/50\n",
            "8/8 [==============================] - 2s 275ms/step - loss: 0.3052 - accuracy: 0.9333\n",
            "Epoch 36/50\n",
            "8/8 [==============================] - 2s 273ms/step - loss: 0.2455 - accuracy: 0.9689\n",
            "Epoch 37/50\n",
            "8/8 [==============================] - 2s 278ms/step - loss: 0.2545 - accuracy: 0.9556\n",
            "Epoch 38/50\n",
            "8/8 [==============================] - 2s 270ms/step - loss: 0.2369 - accuracy: 0.9733\n",
            "Epoch 39/50\n",
            "8/8 [==============================] - 2s 275ms/step - loss: 0.2777 - accuracy: 0.9467\n",
            "Epoch 40/50\n",
            "8/8 [==============================] - 2s 271ms/step - loss: 0.2167 - accuracy: 0.9689\n",
            "Epoch 41/50\n",
            "8/8 [==============================] - 2s 272ms/step - loss: 0.1774 - accuracy: 0.9689\n",
            "Epoch 42/50\n",
            "8/8 [==============================] - 2s 277ms/step - loss: 0.1342 - accuracy: 0.9956\n",
            "Epoch 43/50\n",
            "8/8 [==============================] - 2s 276ms/step - loss: 0.1374 - accuracy: 0.9822\n",
            "Epoch 44/50\n",
            "8/8 [==============================] - 2s 272ms/step - loss: 0.1190 - accuracy: 0.9911\n",
            "Epoch 45/50\n",
            "8/8 [==============================] - 2s 274ms/step - loss: 0.1035 - accuracy: 0.9956\n",
            "Epoch 46/50\n",
            "8/8 [==============================] - 2s 279ms/step - loss: 0.0932 - accuracy: 0.9867\n",
            "Epoch 47/50\n",
            "8/8 [==============================] - 2s 276ms/step - loss: 0.0864 - accuracy: 0.9911\n",
            "Epoch 48/50\n",
            "8/8 [==============================] - 2s 272ms/step - loss: 0.0757 - accuracy: 0.9956\n",
            "Epoch 49/50\n",
            "8/8 [==============================] - 2s 271ms/step - loss: 0.0611 - accuracy: 1.0000\n",
            "Epoch 50/50\n",
            "8/8 [==============================] - 2s 272ms/step - loss: 0.0738 - accuracy: 0.9867\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb926760b00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "0.1\n",
            "0.1268421052631579\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         2\n",
            "           1       0.00      0.00      0.00         0\n",
            "           2       0.38      0.27      0.32        11\n",
            "           3       0.00      0.00      0.00         6\n",
            "           4       0.00      0.00      0.00         0\n",
            "           5       0.00      0.00      0.00         0\n",
            "           6       0.00      0.00      0.00         1\n",
            "           7       1.00      0.25      0.40         4\n",
            "           8       0.00      0.00      0.00         0\n",
            "           9       0.00      0.00      0.00         5\n",
            "          10       0.00      0.00      0.00        11\n",
            "          11       0.00      0.00      0.00         0\n",
            "\n",
            "    accuracy                           0.10        40\n",
            "   macro avg       0.11      0.04      0.06        40\n",
            "weighted avg       0.20      0.10      0.13        40\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "distinguished-arbitration"
      },
      "source": [],
      "id": "distinguished-arbitration",
      "execution_count": null,
      "outputs": []
    }
  ]
}